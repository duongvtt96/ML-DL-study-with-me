{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sharing-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exotic-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5172, 5172)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1. Load the dataset into variables\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "emails = [] # Mail texts\n",
    "labels = [] # Labels(0 for ham, 1 for spam)\n",
    "\n",
    "# Load ham mails\n",
    "for filename in sorted(glob.glob('enron1/ham/*.txt')):\n",
    "    with open(filename, 'r', encoding='ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())\n",
    "    labels.append(0)\n",
    "\n",
    "# Load spam mails\n",
    "for filename in sorted(glob.glob('enron1/spam/*.txt')):\n",
    "    with open(filename, 'r', encoding='ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())\n",
    "    labels.append(1)\n",
    "\n",
    "len(emails), len(labels) # Print out how much data we've loaded is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "little-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Clean the texts\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "def letters_only(astr):\n",
    "    return astr.isalpha()\n",
    "\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(\n",
    "            ' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                     for word in doc.split()\n",
    "                         if letters_only(word)\n",
    "                              and word not in all_names]))\n",
    "    return cleaned_docs\n",
    "            \n",
    "cleaned_emails = clean_text(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: christmas tree farm pictures\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First email before cleaning\n",
    "emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proud-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'christmas tree farm picture'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First email after cleaning\n",
    "cleaned_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Split the dataset into train dataset and test dataset\n",
    "#----------------------------------------------------------------------\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "diagnostic-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Remove unused words and convert the dataset into a Document-Term Matrix\n",
    "#----------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english', max_features=500) # the input text is in English, consider most frequent 500 words only\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "term_docs_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "studied-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 (TODO). Apply the machine learning with a MultinomialNB object\n",
    "#----------------------------------------------------------------------\n",
    "# To achieve the goal:\n",
    "# 1. Understand the entire codes\n",
    "# 2. Search for MultinomialNB class and learn about it\n",
    "# 3. Write correct codes below this comment\n",
    "# 4. The code should train a Naive Bayes model and print out the model accuracy with the test dataset\n",
    "# The expected model accuracy is 0.9197422378441711\n",
    "#\n",
    "# Hint1: Create a MultinomialNB object with the parameters: alpha=1.0 and fit_prior=True\n",
    "# Hint2: Use fit() and score() method to get a result\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# TODO: Write codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9179847685998829"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 (TODO). Apply the machine learning with a MultinomialNB object with the parameters: alpha=1.0 and fit_prior=True.\n",
    "#Print out the model accuracy with the test dataset\n",
    "#----------------------------------------------------------------------\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True) #call MultinomialNB\n",
    "clf.fit(term_docs_train, Y_train) #fit the model with tranining data\n",
    "clf.score(term_docs_test, Y_test) #get mean accuracy on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1138   91]\n",
      " [  49  429]]\n"
     ]
    }
   ],
   "source": [
    "#predict from testing det and compute confusion matrix\n",
    "Y_predict = clf.predict(term_docs_test)\n",
    "cfs_mat = confusion_matrix(Y_test, Y_predict)\n",
    "print(cfs_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (conda)",
   "name": "python383jvsc74a57bd042588fd11209419b7c81abe23e31a1d2101f60811b02170c268e57ebb57d5d9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0a44d1f44045d16b65c891e1489197c1175c98a7d9955d3432e6e4f0c8cef2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

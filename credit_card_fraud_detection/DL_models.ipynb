{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. read and  process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('default of credit card clients.xls', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype\n",
      "---  ------                      --------------  -----\n",
      " 0   ID                          30000 non-null  int64\n",
      " 1   LIMIT_BAL                   30000 non-null  int64\n",
      " 2   SEX                         30000 non-null  int64\n",
      " 3   EDUCATION                   30000 non-null  int64\n",
      " 4   MARRIAGE                    30000 non-null  int64\n",
      " 5   AGE                         30000 non-null  int64\n",
      " 6   PAY_0                       30000 non-null  int64\n",
      " 7   PAY_2                       30000 non-null  int64\n",
      " 8   PAY_3                       30000 non-null  int64\n",
      " 9   PAY_4                       30000 non-null  int64\n",
      " 10  PAY_5                       30000 non-null  int64\n",
      " 11  PAY_6                       30000 non-null  int64\n",
      " 12  BILL_AMT1                   30000 non-null  int64\n",
      " 13  BILL_AMT2                   30000 non-null  int64\n",
      " 14  BILL_AMT3                   30000 non-null  int64\n",
      " 15  BILL_AMT4                   30000 non-null  int64\n",
      " 16  BILL_AMT5                   30000 non-null  int64\n",
      " 17  BILL_AMT6                   30000 non-null  int64\n",
      " 18  PAY_AMT1                    30000 non-null  int64\n",
      " 19  PAY_AMT2                    30000 non-null  int64\n",
      " 20  PAY_AMT3                    30000 non-null  int64\n",
      " 21  PAY_AMT4                    30000 non-null  int64\n",
      " 22  PAY_AMT5                    30000 non-null  int64\n",
      " 23  PAY_AMT6                    30000 non-null  int64\n",
      " 24  default payment next month  30000 non-null  int64\n",
      "dtypes: int64(25)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ID'], inplace=True)\n",
    "df.rename(columns={'PAY_0':'PAY_1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>167484.322667</td>\n",
       "      <td>1.603733</td>\n",
       "      <td>1.853133</td>\n",
       "      <td>1.551867</td>\n",
       "      <td>35.485500</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>-0.133767</td>\n",
       "      <td>-0.166200</td>\n",
       "      <td>-0.220667</td>\n",
       "      <td>-0.266200</td>\n",
       "      <td>...</td>\n",
       "      <td>43262.948967</td>\n",
       "      <td>40311.400967</td>\n",
       "      <td>38871.760400</td>\n",
       "      <td>5663.580500</td>\n",
       "      <td>5.921163e+03</td>\n",
       "      <td>5225.68150</td>\n",
       "      <td>4826.076867</td>\n",
       "      <td>4799.387633</td>\n",
       "      <td>5215.502567</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>129747.661567</td>\n",
       "      <td>0.489129</td>\n",
       "      <td>0.790349</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>9.217904</td>\n",
       "      <td>1.123802</td>\n",
       "      <td>1.197186</td>\n",
       "      <td>1.196868</td>\n",
       "      <td>1.169139</td>\n",
       "      <td>1.133187</td>\n",
       "      <td>...</td>\n",
       "      <td>64332.856134</td>\n",
       "      <td>60797.155770</td>\n",
       "      <td>59554.107537</td>\n",
       "      <td>16563.280354</td>\n",
       "      <td>2.304087e+04</td>\n",
       "      <td>17606.96147</td>\n",
       "      <td>15666.159744</td>\n",
       "      <td>15278.305679</td>\n",
       "      <td>17777.465775</td>\n",
       "      <td>0.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2326.750000</td>\n",
       "      <td>1763.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>8.330000e+02</td>\n",
       "      <td>390.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>117.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19052.000000</td>\n",
       "      <td>18104.500000</td>\n",
       "      <td>17071.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1800.00000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54506.000000</td>\n",
       "      <td>50190.500000</td>\n",
       "      <td>49198.250000</td>\n",
       "      <td>5006.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4505.00000</td>\n",
       "      <td>4013.250000</td>\n",
       "      <td>4031.500000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891586.000000</td>\n",
       "      <td>927171.000000</td>\n",
       "      <td>961664.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>896040.00000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            LIMIT_BAL           SEX     EDUCATION      MARRIAGE           AGE  \\\n",
       "count    30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean    167484.322667      1.603733      1.853133      1.551867     35.485500   \n",
       "std     129747.661567      0.489129      0.790349      0.521970      9.217904   \n",
       "min      10000.000000      1.000000      0.000000      0.000000     21.000000   \n",
       "25%      50000.000000      1.000000      1.000000      1.000000     28.000000   \n",
       "50%     140000.000000      2.000000      2.000000      2.000000     34.000000   \n",
       "75%     240000.000000      2.000000      2.000000      2.000000     41.000000   \n",
       "max    1000000.000000      2.000000      6.000000      3.000000     79.000000   \n",
       "\n",
       "              PAY_1         PAY_2         PAY_3         PAY_4         PAY_5  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.016700     -0.133767     -0.166200     -0.220667     -0.266200   \n",
       "std        1.123802      1.197186      1.196868      1.169139      1.133187   \n",
       "min       -2.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        8.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
       "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
       "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
       "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
       "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
       "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
       "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
       "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
       "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
       "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
       "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
       "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
       "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
       "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default payment next month  \n",
       "count   30000.000000                30000.000000  \n",
       "mean     5215.502567                    0.221200  \n",
       "std     17777.465775                    0.415062  \n",
       "min         0.000000                    0.000000  \n",
       "25%       117.750000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4000.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for sex, education, marriage and pay\n",
    "df = pd.get_dummies(df, prefix=['SEX', 'EDU', 'MARRIAGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'], columns=['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "df_X = df.drop(['default payment next month'], axis=1)\n",
    "df_y = df[['default payment next month']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_scale = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "       'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "       'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize features \n",
    "from sklearn import preprocessing\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "X_train[col_to_scale] = std_scaler.fit_transform(X_train[col_to_scale])\n",
    "X_test[col_to_scale] = std_scaler.transform(X_test[col_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 0s 819us/step - loss: 0.4783 - accuracy: 0.7795\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4374 - accuracy: 0.8159\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4347 - accuracy: 0.8191\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 0s 715us/step - loss: 0.4339 - accuracy: 0.8198\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4325 - accuracy: 0.8198\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 0s 727us/step - loss: 0.4311 - accuracy: 0.8208\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 0s 731us/step - loss: 0.4298 - accuracy: 0.8212\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 0s 718us/step - loss: 0.4296 - accuracy: 0.8202\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 0s 746us/step - loss: 0.4291 - accuracy: 0.8213\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 0s 699us/step - loss: 0.4283 - accuracy: 0.8206\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.4274 - accuracy: 0.8210\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 0s 720us/step - loss: 0.4273 - accuracy: 0.8210\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4261 - accuracy: 0.8208\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - 0s 733us/step - loss: 0.4258 - accuracy: 0.8207\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - 0s 719us/step - loss: 0.4256 - accuracy: 0.8217\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4246 - accuracy: 0.8216\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - 0s 727us/step - loss: 0.4248 - accuracy: 0.8212\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - 0s 720us/step - loss: 0.4238 - accuracy: 0.8225\n",
      "Epoch 19/100\n",
      "188/188 [==============================] - 0s 732us/step - loss: 0.4234 - accuracy: 0.8224\n",
      "Epoch 20/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4231 - accuracy: 0.8220\n",
      "Epoch 21/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4228 - accuracy: 0.8224\n",
      "Epoch 22/100\n",
      "188/188 [==============================] - 0s 727us/step - loss: 0.4227 - accuracy: 0.8222\n",
      "Epoch 23/100\n",
      "188/188 [==============================] - 0s 711us/step - loss: 0.4226 - accuracy: 0.8230\n",
      "Epoch 24/100\n",
      "188/188 [==============================] - 0s 722us/step - loss: 0.4218 - accuracy: 0.8225\n",
      "Epoch 25/100\n",
      "188/188 [==============================] - 0s 710us/step - loss: 0.4217 - accuracy: 0.8233\n",
      "Epoch 26/100\n",
      "188/188 [==============================] - 0s 727us/step - loss: 0.4211 - accuracy: 0.8220\n",
      "Epoch 27/100\n",
      "188/188 [==============================] - 0s 725us/step - loss: 0.4209 - accuracy: 0.8228\n",
      "Epoch 28/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4206 - accuracy: 0.8221\n",
      "Epoch 29/100\n",
      "188/188 [==============================] - 0s 720us/step - loss: 0.4203 - accuracy: 0.8227\n",
      "Epoch 30/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4199 - accuracy: 0.8237\n",
      "Epoch 31/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4195 - accuracy: 0.8232\n",
      "Epoch 32/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4191 - accuracy: 0.8235\n",
      "Epoch 33/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4187 - accuracy: 0.8241\n",
      "Epoch 34/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4183 - accuracy: 0.8238\n",
      "Epoch 35/100\n",
      "188/188 [==============================] - 0s 716us/step - loss: 0.4175 - accuracy: 0.8248\n",
      "Epoch 36/100\n",
      "188/188 [==============================] - 0s 733us/step - loss: 0.4180 - accuracy: 0.8250\n",
      "Epoch 37/100\n",
      "188/188 [==============================] - 0s 725us/step - loss: 0.4172 - accuracy: 0.8255\n",
      "Epoch 38/100\n",
      "188/188 [==============================] - 0s 731us/step - loss: 0.4177 - accuracy: 0.8242\n",
      "Epoch 39/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4169 - accuracy: 0.8249\n",
      "Epoch 40/100\n",
      "188/188 [==============================] - 0s 715us/step - loss: 0.4165 - accuracy: 0.8248\n",
      "Epoch 41/100\n",
      "188/188 [==============================] - 0s 713us/step - loss: 0.4161 - accuracy: 0.8246\n",
      "Epoch 42/100\n",
      "188/188 [==============================] - 0s 725us/step - loss: 0.4164 - accuracy: 0.8244\n",
      "Epoch 43/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4154 - accuracy: 0.8260\n",
      "Epoch 44/100\n",
      "188/188 [==============================] - 0s 728us/step - loss: 0.4154 - accuracy: 0.8250\n",
      "Epoch 45/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4148 - accuracy: 0.8252\n",
      "Epoch 46/100\n",
      "188/188 [==============================] - 0s 716us/step - loss: 0.4152 - accuracy: 0.8260\n",
      "Epoch 47/100\n",
      "188/188 [==============================] - 0s 718us/step - loss: 0.4146 - accuracy: 0.8258\n",
      "Epoch 48/100\n",
      "188/188 [==============================] - 0s 736us/step - loss: 0.4141 - accuracy: 0.8262\n",
      "Epoch 49/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4134 - accuracy: 0.8257\n",
      "Epoch 50/100\n",
      "188/188 [==============================] - 0s 734us/step - loss: 0.4134 - accuracy: 0.8262\n",
      "Epoch 51/100\n",
      "188/188 [==============================] - 0s 714us/step - loss: 0.4131 - accuracy: 0.8272\n",
      "Epoch 52/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4124 - accuracy: 0.8264\n",
      "Epoch 53/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4125 - accuracy: 0.8268\n",
      "Epoch 54/100\n",
      "188/188 [==============================] - 0s 711us/step - loss: 0.4121 - accuracy: 0.8279\n",
      "Epoch 55/100\n",
      "188/188 [==============================] - 0s 728us/step - loss: 0.4119 - accuracy: 0.8270\n",
      "Epoch 56/100\n",
      "188/188 [==============================] - 0s 711us/step - loss: 0.4114 - accuracy: 0.8276\n",
      "Epoch 57/100\n",
      "188/188 [==============================] - 0s 714us/step - loss: 0.4109 - accuracy: 0.8276\n",
      "Epoch 58/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4102 - accuracy: 0.8272\n",
      "Epoch 59/100\n",
      "188/188 [==============================] - 0s 712us/step - loss: 0.4103 - accuracy: 0.8282\n",
      "Epoch 60/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4099 - accuracy: 0.8280\n",
      "Epoch 61/100\n",
      "188/188 [==============================] - 0s 725us/step - loss: 0.4093 - accuracy: 0.8280\n",
      "Epoch 62/100\n",
      "188/188 [==============================] - 0s 719us/step - loss: 0.4093 - accuracy: 0.8287\n",
      "Epoch 63/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.4089 - accuracy: 0.8281\n",
      "Epoch 64/100\n",
      "188/188 [==============================] - 0s 719us/step - loss: 0.4091 - accuracy: 0.8292\n",
      "Epoch 65/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4090 - accuracy: 0.8290\n",
      "Epoch 66/100\n",
      "188/188 [==============================] - 0s 720us/step - loss: 0.4073 - accuracy: 0.8294\n",
      "Epoch 67/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.4072 - accuracy: 0.8303\n",
      "Epoch 68/100\n",
      "188/188 [==============================] - 0s 732us/step - loss: 0.4077 - accuracy: 0.8291\n",
      "Epoch 69/100\n",
      "188/188 [==============================] - 0s 718us/step - loss: 0.4069 - accuracy: 0.8304\n",
      "Epoch 70/100\n",
      "188/188 [==============================] - 0s 732us/step - loss: 0.4067 - accuracy: 0.8302\n",
      "Epoch 71/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4067 - accuracy: 0.8292\n",
      "Epoch 72/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4054 - accuracy: 0.8303\n",
      "Epoch 73/100\n",
      "188/188 [==============================] - 0s 714us/step - loss: 0.4053 - accuracy: 0.8304\n",
      "Epoch 74/100\n",
      "188/188 [==============================] - 0s 713us/step - loss: 0.4048 - accuracy: 0.8303\n",
      "Epoch 75/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.4053 - accuracy: 0.8304\n",
      "Epoch 76/100\n",
      "188/188 [==============================] - 0s 717us/step - loss: 0.4052 - accuracy: 0.8302\n",
      "Epoch 77/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.4045 - accuracy: 0.8302\n",
      "Epoch 78/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4043 - accuracy: 0.8317\n",
      "Epoch 79/100\n",
      "188/188 [==============================] - 0s 722us/step - loss: 0.4035 - accuracy: 0.8307\n",
      "Epoch 80/100\n",
      "188/188 [==============================] - 0s 735us/step - loss: 0.4034 - accuracy: 0.8317\n",
      "Epoch 81/100\n",
      "188/188 [==============================] - 0s 731us/step - loss: 0.4024 - accuracy: 0.8322\n",
      "Epoch 82/100\n",
      "188/188 [==============================] - 0s 734us/step - loss: 0.4026 - accuracy: 0.8315\n",
      "Epoch 83/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.4024 - accuracy: 0.8307\n",
      "Epoch 84/100\n",
      "188/188 [==============================] - 0s 730us/step - loss: 0.4019 - accuracy: 0.8308\n",
      "Epoch 85/100\n",
      "188/188 [==============================] - 0s 722us/step - loss: 0.4021 - accuracy: 0.8320\n",
      "Epoch 86/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4017 - accuracy: 0.8305\n",
      "Epoch 87/100\n",
      "188/188 [==============================] - 0s 724us/step - loss: 0.4011 - accuracy: 0.8315\n",
      "Epoch 88/100\n",
      "188/188 [==============================] - 0s 732us/step - loss: 0.4006 - accuracy: 0.8326\n",
      "Epoch 89/100\n",
      "188/188 [==============================] - 0s 731us/step - loss: 0.4001 - accuracy: 0.8325\n",
      "Epoch 90/100\n",
      "188/188 [==============================] - 0s 726us/step - loss: 0.3996 - accuracy: 0.8318\n",
      "Epoch 91/100\n",
      "188/188 [==============================] - 0s 729us/step - loss: 0.4000 - accuracy: 0.8327\n",
      "Epoch 92/100\n",
      "188/188 [==============================] - 0s 732us/step - loss: 0.3992 - accuracy: 0.8328\n",
      "Epoch 93/100\n",
      "188/188 [==============================] - 0s 719us/step - loss: 0.3997 - accuracy: 0.8322\n",
      "Epoch 94/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.3990 - accuracy: 0.8331\n",
      "Epoch 95/100\n",
      "188/188 [==============================] - 0s 721us/step - loss: 0.3982 - accuracy: 0.8328\n",
      "Epoch 96/100\n",
      "188/188 [==============================] - 0s 723us/step - loss: 0.3984 - accuracy: 0.8344\n",
      "Epoch 97/100\n",
      "188/188 [==============================] - 0s 733us/step - loss: 0.3974 - accuracy: 0.8341\n",
      "Epoch 98/100\n",
      "188/188 [==============================] - 0s 726us/step - loss: 0.3972 - accuracy: 0.8339\n",
      "Epoch 99/100\n",
      "188/188 [==============================] - 0s 719us/step - loss: 0.3970 - accuracy: 0.8325\n",
      "Epoch 100/100\n",
      "188/188 [==============================] - 0s 720us/step - loss: 0.3971 - accuracy: 0.8340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f728142dfd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 422us/step - loss: 0.4342 - accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43423983454704285, 0.8199999928474426]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for simpleRNN: \n",
      " [[4420  263]\n",
      " [ 817  500]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (rnn_model.predict(X_test).ravel()>0.5)+0 \n",
    "print('\\nConfusion matrix for simpleRNN: \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. LSTM\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(units=50, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(50, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7951\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.8158\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.8191\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8196\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8203\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8220\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8213\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.8221\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8231\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8225\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8226\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8228\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8223\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8223\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.8233\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8229\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8236\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8242\n",
      "Epoch 19/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8242\n",
      "Epoch 20/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8235\n",
      "Epoch 21/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8244\n",
      "Epoch 22/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8245\n",
      "Epoch 23/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8246\n",
      "Epoch 24/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8248\n",
      "Epoch 25/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8242\n",
      "Epoch 26/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8243\n",
      "Epoch 27/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8255\n",
      "Epoch 28/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8249\n",
      "Epoch 29/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8249\n",
      "Epoch 30/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8253\n",
      "Epoch 31/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8250\n",
      "Epoch 32/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8249\n",
      "Epoch 33/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8264\n",
      "Epoch 34/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8257\n",
      "Epoch 35/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8260\n",
      "Epoch 36/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8271\n",
      "Epoch 37/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8273\n",
      "Epoch 38/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8265\n",
      "Epoch 39/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8276\n",
      "Epoch 40/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8276\n",
      "Epoch 41/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8292\n",
      "Epoch 42/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8282\n",
      "Epoch 43/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8284\n",
      "Epoch 44/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8270\n",
      "Epoch 45/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.8285\n",
      "Epoch 46/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8295\n",
      "Epoch 47/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8289\n",
      "Epoch 48/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8277\n",
      "Epoch 49/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8287\n",
      "Epoch 50/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8289\n",
      "Epoch 51/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8304\n",
      "Epoch 52/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8298\n",
      "Epoch 53/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.8303\n",
      "Epoch 54/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8297\n",
      "Epoch 55/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8287\n",
      "Epoch 56/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8302\n",
      "Epoch 57/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8305\n",
      "Epoch 58/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8307\n",
      "Epoch 59/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8310\n",
      "Epoch 60/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8309\n",
      "Epoch 61/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8310\n",
      "Epoch 62/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8310\n",
      "Epoch 63/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8317\n",
      "Epoch 64/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8313\n",
      "Epoch 65/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8315\n",
      "Epoch 66/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8322\n",
      "Epoch 67/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8313\n",
      "Epoch 68/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8334\n",
      "Epoch 69/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8313\n",
      "Epoch 70/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8328\n",
      "Epoch 71/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8325\n",
      "Epoch 72/100\n",
      "188/188 [==============================] - 0s 3ms/step - loss: 0.4076 - accuracy: 0.8335\n",
      "Epoch 73/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8323\n",
      "Epoch 74/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8345\n",
      "Epoch 75/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8325\n",
      "Epoch 76/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8327\n",
      "Epoch 77/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8336\n",
      "Epoch 78/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8330\n",
      "Epoch 79/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8325\n",
      "Epoch 81/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8336\n",
      "Epoch 82/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8330\n",
      "Epoch 83/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8349\n",
      "Epoch 84/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8347\n",
      "Epoch 85/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8362\n",
      "Epoch 86/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8343\n",
      "Epoch 87/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8356\n",
      "Epoch 88/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8337\n",
      "Epoch 89/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8346\n",
      "Epoch 90/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8344\n",
      "Epoch 91/100\n",
      "188/188 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8355\n",
      "Epoch 92/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8359\n",
      "Epoch 93/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8349\n",
      "Epoch 94/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8358\n",
      "Epoch 95/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8355\n",
      "Epoch 96/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8354\n",
      "Epoch 97/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8352\n",
      "Epoch 98/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8364\n",
      "Epoch 99/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8363\n",
      "Epoch 100/100\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f701065db70>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 689us/step - loss: 0.4423 - accuracy: 0.8218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44233763217926025, 0.8218333125114441]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "lstm_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for LSTM: \n",
      " [[4485  198]\n",
      " [ 871  446]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (lstm_model.predict(X_test).ravel()>0.5)+0\n",
    "print('\\nConfusion matrix for LSTM: \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. GRU\n",
    "from keras.optimizers import SGD\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh'))\n",
    "gru_model.add(Dropout(0.2))\n",
    "gru_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "gru_model.add(Dropout(0.2))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "gru_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7745\n",
      "Epoch 2/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7915\n",
      "Epoch 3/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.8064\n",
      "Epoch 4/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8093\n",
      "Epoch 5/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8129\n",
      "Epoch 6/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8152\n",
      "Epoch 7/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8161\n",
      "Epoch 8/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8175\n",
      "Epoch 9/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.8173\n",
      "Epoch 10/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8171\n",
      "Epoch 11/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8179\n",
      "Epoch 12/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8182\n",
      "Epoch 13/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8185\n",
      "Epoch 14/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8187\n",
      "Epoch 15/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8171\n",
      "Epoch 16/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8183\n",
      "Epoch 17/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8192\n",
      "Epoch 18/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8191\n",
      "Epoch 19/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8192\n",
      "Epoch 20/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8192\n",
      "Epoch 21/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4323 - accuracy: 0.8203\n",
      "Epoch 22/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8206\n",
      "Epoch 23/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.8188\n",
      "Epoch 24/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8192\n",
      "Epoch 25/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8197\n",
      "Epoch 26/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8195\n",
      "Epoch 27/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8195\n",
      "Epoch 28/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8191\n",
      "Epoch 29/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8199\n",
      "Epoch 30/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8202\n",
      "Epoch 31/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8194\n",
      "Epoch 32/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8206\n",
      "Epoch 33/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8200\n",
      "Epoch 34/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8193\n",
      "Epoch 35/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8197\n",
      "Epoch 36/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8208\n",
      "Epoch 37/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8195\n",
      "Epoch 38/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8202\n",
      "Epoch 39/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8202\n",
      "Epoch 40/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8200\n",
      "Epoch 41/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8196\n",
      "Epoch 42/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8205\n",
      "Epoch 43/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8197\n",
      "Epoch 44/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8185\n",
      "Epoch 45/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8206\n",
      "Epoch 46/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8212\n",
      "Epoch 47/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8195\n",
      "Epoch 48/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8199\n",
      "Epoch 49/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.8204\n",
      "Epoch 50/50\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70a057d630>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.fit(X_train,y_train,epochs=50,batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 670us/step - loss: 0.4219 - accuracy: 0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4218994677066803, 0.8258333206176758]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for GRU: \n",
      " [[4466  217]\n",
      " [ 828  489]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (gru_model.predict(X_test).ravel()>0.5)+0\n",
    "print('\\nConfusion matrix for GRU: \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python373jvsc74a57bd04819e79a9fda6cc6287b86e364e3ac4dcab40819af21efc766dc509c35541405"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
